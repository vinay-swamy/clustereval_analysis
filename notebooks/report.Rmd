---
title: "clustereval: evaluating graph based clustering"
output: html_notebook
---


```{r setup, echo = F}
knitr::opts_chunk$set(echo = F, warning = F, message = F)
```

# Introduction

  Community detection, a type of clustering, is a common step in the analysis of computational graphs and networks. In community detection, the nodes of a graph are divided into different partitions; this is achieved by optimizing a modularity function. Modularity is roughly a measure of how connected a specific partition of a graph is connected to itself vs the rest of the graph.

This is the basic formula for modularity
$$ Q = \frac{1}{2m}\sum_{c} (M_c - \frac{K_c^2}{4m})  $$
where *M_c* is the number of internal edges in a patition  *c* ( edge strictly between nodes in *c*),
*K_c^2* is the total number of edges associated with nodes in *c*, and *m* is the total number of edges in the graph. Therefore, an ideal community *c* where Q=1 would only have connections to itself and be completely disconnected to the rest of the graph. For most algorithmns that optimize modularity (louvain, leiden, etc), the number of partitions, analgous to the number of clusters in conventional clustering, is deterministic, when using this definition of modularity 

  A more commonly used version of modularity(Reichardt and Bornholdt's model) is 
$$ Q = \frac{1}{2m}\sum_{c} (M_c - \gamma\frac{K_c^2}{4m})  $$ 
where $$ \gamma $$ is the resolution parameter, which can tune the numbern of partitions detected by the partitioning algorithmn. Just like in convential clustering, the choice of how many partitions to generate is a non-trivial task. As there is often no ground truth in determing the true number of clusters, it is difficult to say which set of labels identified by clustering are the most accurate.

  
## Metrics for determining clustering/partitioning accuracy.
  There have been multiple metrics proposed for evaluating clustering accuracy. When ground truth labels exist, label based methods like the Rand index or normalized Mutual information work well. However, most of the time this is not the case. Ground-truth free methods for evaluating clustering accuracy like the silhouette coefficient or Daviesâ€“Bouldin index are often distance based, but distance is not as easily defined for graphs. 
  von Luxburg et al. proposes a more general approach for evaluating clustering: the "perturbation experiment". In a perturbation experiment, some amount of noise is added to a subset of the data; this perturbed data is then re-clustered. This is repeated multiple times, and then the original, non-perturbed dataset is compared against the peturbed ones.  This experiment can be repeated across different hyperparameters for a clustering algorithmn. The extent to which a particular clustering remains accurate over a set of perturbations is its immutability(I), or more formally, 
  $$ I(hp, n) = \frac1n\sum_{n=0} d(C, C_n) $$
where *hp* represents the set of hyperparamters used to generate non-perturbed clustering *C*,
*n* = number of pertrubations 
*C_n* is clustering from perturbed data, 
and *d(...)* is a method for measuring the distance between two sets of clusters. 

In this project, graphs can be perturbed by  randomly adding and removing edges, or randomly changing edge weights.

Perturbation experiments are done across multiple sets of hyperparameters, and the set of HP which maximize *I* are the "best" hyper parameters. Note that this procedure is really answering the questions "How sensitve is a proposed set of labels to small changes in the input data"

### Measuring the distance between two sets of clusters 
  Any label based measure of accuracy can be used, ie those mentioned above. However, for this project, we consider two metrics previously described by Shekar et al., Stability(*S*) and Purity(*P*), which adapt label-based metrics specifcally for use in pertubation experiments.
  
#### Stability
Stability S calculated per-cluster for the non-perturbed data and based on shannon entropy. Keep in mind that the set of observations will be the same across non-perturbed and perturbed data, but perturbed data has permuted features.
$$ S_k = 1- \frac{1}{n}\sum_{i=1}^n \frac{H_i^k}{H_i^{Tot}} $$
where *i* is a perturbed dataset, *n* is the total number of rounds of perturbation, and *k* is a single cluster within the non-perturbed dataset.

H is the shannon entropy of a set of labels for data : $$ H = -\sum_c^C f_c * ln(f_c) $$, where:
*f_c* is the fraction of observations in a dataset that have label *c*, and *C* is the total set of labels for this dataset. In this above stability equation, $$H_i^{Tot}$$ is the shannon entropy for a perturbed dataset *i*. For $$H_i^k$$, the  perturbed dataset *i* subset to contain only the observations assigned to label *k* in the non-perturbed dataset, and then the shannon entropy is calculated for this subset. *S_k* is the average of $$ \frac{H_i^k}{H_i^{Tot}}$$ across all perturbations. And so each cluster in the non-perturbed labels will ha ve its own S_k 

#### Purity 

Let O be the cluster j in perturbation experiment i that has the highest overlap with non-perturbed cluster k.  $$ O_j =   max_{j=1}( NonPerturbed_k \in Perturbed_j) $$ 

Purity is then

$$ P_k = \frac{1}{n} \sum_{i=1}^{n} \frac{len(O_j)}{len({Perturbed_j})} $$

 Purity is the the fraction of observations that in a reference cluster k that maximally overlaps with a cluster perturbation experiment. 
 
 
 
### An Example 

```{r include=F, echo = F}
library(tidyverse)
library(patchwork)
n =100
df <- bind_rows(
    tibble(x= rnorm(n, 5), y=rnorm(n, 5), clustering_1 = 'A'),
    tibble(x= rnorm(n, -5), y=rnorm(n, 5), clustering_1 = 'B'),
    tibble(x= rnorm(n, -5), y=rnorm(n, -5), clustering_1 = 'C')

)

lab_df = tibble(x =c(-5,5, -5, 5), y=c(5, 5, -5, -5), lab = c('1', '2', '3', '4'))
main_clu <- ggplot(df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df[1:3,], aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('reference')+
    theme_minimal()

merge_df <- df
merge_df[merge_df$clustering_1 == 'C',]$x <- rnorm(n, 5)
merge_df[merge_df$clustering_1 == 'C',]$y <- rnorm(n, 5)

merge_clu <- ggplot() + 
    geom_point( data = merge_df, aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df[1:2,], aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Stable, but not Pure')+
    theme_minimal()


split_df <- df 
split_df$x[251:300] <- rnorm(50, 5)
split_df$y[251:300] <- rnorm(50, -5)
split_df$clustering_2A <- c(rep('1', 100),rep('2', 100), rep('3', 50), rep('4', 50) )
split_clu <- ggplot(split_df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df, aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Pure, but not Stable')+
    theme_minimal()


prom_df <-df[c(1:74, 100:174,200:275),  ] %>% 
    mutate(clustering_2B =c(rep('1', 75),rep('2', 75), rep('3', 75) ) )
prom_df <- bind_rows(prom_df, 
                     tibble(x=rnorm(75, 5), y= rnorm(75, -5),
                     clustering_1 = sample(c('A','B','C'), 75,  replace = T), clustering_2B = '4' )
                    )
prom_clu <- ggplot(prom_df) + 
    geom_point(aes(x=x, y=y, color=clustering_1), size = 3) + 
    geom_text(data =lab_df, aes(x=x, y=y, label = lab), size = 5)+
    xlab('')+
    ylab('')+
    ggtitle('Not pure not Stable')+
    theme_minimal()
```

Here is a plot that describes the behavior of each metric:

```{r, echo =F}
(main_clu + merge_clu) / ( split_clu | prom_clu) +plot_layout(guides='collect' ) & plot_annotation(tag_levels = 'A') & theme_bw() & 
    #geom_vline(xintercept = 0)& geom_hline(yintercept = 0) & 
    xlim(c(-10, 10)) & ylim(c(-10,10))  & guides(color = guide_legend(title = 'Original Cluster'))


```


  In this example, Cluster 3 of the reference cluster is compared in different situations. In A vs B, cluster 3 has been merged with another cluster. The Stability for A-3  is 1, because all observations in A-3 are preserved within a single cluster, B-2. However, the purity of A-3 is low, because the overall size of the maximum overlap cluster, B-2, is larger than the original cluster A-3. Data that has High Stability is likely over clustered, as smaller cluster become merged into another cluster when only a few edges are added/removed

The opposite is true for Panel C. Cluster A-3 splits into two separate clusters, C-2, C-3. Data with high Purity but low stability is likely under-clustered, as the addition/subtraction of a few edges is enough to cause the cluster to split. 

## Graph partitioning of a k-Nearest Neighbor graph

k-NN graphs built from approximating nearest neighbors is a key step in many types of data analyses, but particularly for scRNA-seq. For each observation, the top K closest cells are identified; This can be converted to graph form by making each observation a node, and having edges between nearest neighbors. When building a graph in this method, the number of edges directly depends on k; m = k * n_observations. Note that when using the Reichardt and Bornholdt model of modularity, the resolution parameter can be ignored, because different combinations of resolution and k will produce the same value ie gamma/2(k* n) == 2gamma/2(.5k*n). For this reason, fixing the resolution parameter and only tuning k is a reasonable approach from optimizing nearest-neighbor graph clustering.


## Another Toy Example

```{r}
library(reticulate)
use_python('~/miniconda3/bin/python', required = T)
```

 
```{python}
import pickle 
import pandas as pd
import clustereval as ce
import numpy as np
import glob
import re
import plotnine as pn
import igraph as ig
import leidenalg
import copy 
import louvain
import warnings
warnings.filterwarnings("ignore")
```
 
This is some fake 2D data that I made. 
 
```{python}
sim_data=pd.read_csv('simulated_clusters.csv.gz')
sim_mat = sim_data[['x', 'y']]
(
    pn.ggplot(sim_data) + 
    pn.geom_point(pn.aes(x='x', y='y', color = 'default_label')) + 
    pn.ggtitle('Fake data XY coords,  True labels') +
    pn.theme_minimal()
)
```
 
I make a NN graph from this data, then generate a UMAP reduction using the graph( similar to how we do for scRNA-seq)

```{python}
clu_obj = ce.cluster.ClusterExperiment(sim_mat, 0)
clu_obj.buildNeighborGraph(10,'l2', 150, False, False, None, None )
labels = clu_obj.run_leiden(vertex_partition_method=leidenalg.RBConfigurationVertexPartition, n_iter=5, 
                                                   resolution=1.0, jac_weighted_edges='weight' )
sim_data['no_ptb_clu_labels'] = clu_obj.merge_singletons(labels, 25)
sim_data['no_ptb_clu_labels'] = sim_data['no_ptb_clu_labels'].astype(str) 
clu_umap = pd.DataFrame(clu_obj.run_UMAP()[0], columns = ['UMAP1', 'UMAP2'] ).join(sim_data)
(
    pn.ggplot(clu_umap) + 
    pn.geom_point(pn.aes(x='UMAP1', y='UMAP2', color = 'no_ptb_clu_labels')) + 
    pn.ggtitle('Fake data UMAP coords, true labels') +
    pn.theme_minimal()
)
```
 
Now cluster the graph. This particular run has k=10 with the leiden algorithmn
 
```{python}
(
    pn.ggplot(clu_umap) + 
    pn.geom_point(pn.aes(x='UMAP1', y='UMAP2', color = 'no_ptb_clu_labels')) + 
    pn.ggtitle('Fake data UMAP coords, clustered labels(k=4, alg=leiden)') +
    pn.theme_minimal()
)
```
 
So looks like we're finding more clusters than we should. 
 
 
```{python, include = F}
clu_obj.nn_graph.vs['default_label'] = sim_data['default_label'].to_numpy()
clu_obj.nn_graph.vs['no_ptb_clu_labels'] = sim_data['no_ptb_clu_labels'].to_numpy()
node_alpha= 1
vs_default_labels = {}
vs_default_labels["vertex_size"] = 5
color_dict = {'A': 'red', 
              'B': 'blue',
              'C': 'green', 
              'D': 'yellow', 
              'E': 'orange',
              'F': 'pink',
              'G': 'cyan',
              'H': 'indigo', 
              'I': 'violet'}
vs_default_labels["vertex_color"] = [color_dict[dl] for dl in clu_obj.nn_graph.vs['default_label']]
default_layout = clu_obj.nn_graph.layout()
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ig.plot(clu_obj.nn_graph, layout = default_layout, **vs_default_labels, target='tmp.png')
```
 
We can run a perturbation experiment against this clustering

```{python}
perturb_out = ce.cluster.run_perturbations(clu_obj, 1.0, 'leiden', 30,.05, None, 25,0)
ref_label = pd.DataFrame().assign(Barcode = list(sim_data.index),labels = sim_data['no_ptb_clu_labels']).sort_values('labels')
metrics = ce.metrics.calculate_metrics(ref_label, perturb_out, 'example')
metrics
```

To better look see what exactly is going on in these perturbations, we can look at the NN graph

Normal:
 
```{r}
knitr::include_graphics('tmp.png')
```


```{python, include = F}
ptb_clu_obj = copy.deepcopy(clu_obj)
ptb_clu_obj.run_perturbation(edge_permut_frac=.05)
ig.plot(ptb_clu_obj.nn_graph, layout = default_layout,**vs_default_labels, target='tmp.png' )

```

Pertutbed NN graph

```{r}
knitr::include_graphics('tmp.png')
```
 
Effect of this perturbation on clustering 
 
```{python}
labels=ptb_clu_obj.run_leiden(vertex_partition_method=leidenalg.RBConfigurationVertexPartition, n_iter=5, resolution=1.0, jac_weighted_edges='weight' )
clu_umap['edge_ptb_.05'] = ptb_clu_obj.merge_singletons(labels,25)
clu_umap['edge_ptb_.05']=clu_umap['edge_ptb_.05'].astype(str)
(
    pn.ggplot(clu_umap) + 
    pn.geom_point(pn.aes(x='UMAP1', y='UMAP2', color = 'edge_ptb_.05')) + 
    pn.theme_minimal()
)

```

Now we can try out a different set of parameters: k=30, alg=leiden


```{python}
clu_obj = ce.cluster.ClusterExperiment(sim_mat, 0)
clu_obj.buildNeighborGraph(30,'l2', 150, False, False, None, None )
labels = clu_obj.run_leiden(vertex_partition_method=leidenalg.RBConfigurationVertexPartition, n_iter=5, 
                                                   resolution=1.0, jac_weighted_edges='weight' )
labels = clu_obj.merge_singletons(labels, 25)

clu_umap['no_ptb_knn_30'] = labels.astype(str)
(
    pn.ggplot(clu_umap) + 
    pn.geom_point(pn.aes(x='UMAP1', y='UMAP2', color = 'no_ptb_knn_30')) + 
    pn.theme_minimal()
)
```
 
 
```{python}
perturb_out_knn30 = ce.cluster.run_perturbations(clu_obj, 1.0, 'leiden', 30,.05, None, 25,0)
ref_labels = ref_label = pd.DataFrame().assign(Barcode = list(sim_data.index),labels = clu_umap['no_ptb_knn_30']).sort_values('labels')
metrics = ce.metrics.calculate_metrics(ref_label, perturb_out_knn30, 'example')
metrics 
```

### Trying it out on some real data 

I'm testing this out on two data sets, the PBMC 3K from seurat, and the Sanes Amacrine data from GEO. Both have been preprocessed by the standard seurat pipeline, and clustering is being done on 40 PCs for both. 
Ran perturbation experiment for varying values of k, clustered with louvain/leiden, and then ran perturbation experiments on each cluster iteration. Each experiment randomly removed 5% of edges and randomly added the same amount of edges

#### PBMC data

```{r}
pbmc_metrics <- read_csv('~/Downloads/pbmc_metric_summary.csv.gz') %>% select(-X1)
head(pbmc_metrics, 30)
```

Clear trend where more clusters >  worse performance

PBMC is supposed to have 9 celltypes, so data might be underclustered. Will have come up with some procedure to deal with the

Ideally, would like to find some sort of biological tie

```{r}
library(clustree)
knn98 <- read_csv('~/Downloads/ds-pbmc_alg-louvain_knn-98_.csv.gz')  %>% rename( knn98 = labels) %>% 
  select(Barcode, knn98)
labelled_data <- read_csv('~/Downloads/pbmc_celltypes.csv.gz') %>% rename(knn10000 = cellType)
clustree_df <- inner_join(labelled_data, knn98)
clustree(clustree_df, prefix = 'knn')

```


```{r}
pbmc_embedding <- read_csv('~/Downloads/pbmc_preproccessed.csv.gz') %>% as.data.frame
rownames(pbmc_embedding) <- pbmc_embedding$Barcode
pbmc_embedding <- pbmc_embedding[knn98$Barcode,-1]

data.dims <- lapply( unique(knn98$knn98), function(x) {
    cells <-knn98 %>% filter( .[,'knn98'] == x ) %>% pull(Barcode)
    colMeans(pbmc_embedding[cells, ])
})
data.dims <- do.call(what = "cbind", args = data.dims)
colnames(data.dims) <- unique(knn98$knn98)
data.dist <- dist(x = t(x = data.dims))


plot(hclust(data.dist))
```





#### Sanes data 

```{r}
amacrine_metrics <- read_csv('~/Downloads/sanes_amacrine_metric_summary.csv.gz') %>% select(-X1)

head(amacrine_metrics,, 30)
```

For the sanes data, the  scores are a little worse, but interestingly  score is not as correlated with cluster size( ranges between 43-132 ). 






 
 